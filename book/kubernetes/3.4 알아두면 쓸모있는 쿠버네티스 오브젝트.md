# 알아두면 쓸모있는 쿠버네티스 오브젝트
지금까지 파드를 안정적으로 사용하는 방법을 배우며 파드를 관리하는 여러 가지 기능이 포함된 디플로이먼트 오브젝트를 사용해 봤다.
디플로이먼트 외에도 용도에 따라 사용할 수 있는 다양한 오브젝트가 이미 정의돼 있다. (데몬셋, 컨피그맵, PV, PVC, 스테이트풀셋 등)

## ✅ 데몬셋
`데몬셋 (DaemonSet)`: 디플로이먼트의 replicas 가 노드 수만큼 정해져 있는 형태라고 할 수 있는데, 노드 하나당 파드 한 개만을 생성한다. 
결국 노드를 관리하는 파드라면 데몬셋으로 만드는 게 가장 효율적이다.

실습으로 데몬셋을 만들어 보면서 데몬셋의 작동 원리를 확인한다.

```shell
# m-k8s at super putty
kubectl get pods -n metallb-system -o wide
```
1. 현재 MetalLB의 스피커가 각 노드에 분포돼 있는 상태를 확인한다.

```shell
# Vagrantfile
Vagrant.configure("2") do |config|
  N = 4 # max number of worker nodes
  Ver = '1.18.4' # Kubernetes Version to install

  #=============#
  # Master Node #
  #=============#
```
2. 워커 노드를 4개로 변경한다.

```shell
# CMD of host computer
cd C:\HashiCorp\_Book_k8sInfra-main\ch3\3.1.3 
vagrant up w4-k8s
```
3. 호스트 컴퓨터 명령 창에서 C:\HashiCorp\_Book_k8sInfra-main\ch3\3.1.3 경로로 이동한다.
4. 새로운 워커노드를 추가하는 명령을 실행한다.

```shell
# m-k8s at super putty
kubectl get pods -n metallb-system -o wide -w
kubectl get pods speaker-vnc2k -o yaml -n metallb-system
```
5. w4-k8s 가 추가되면 m-k8s 에서 kubectl get pods -n metallb-system -o wide -w 를 수행한다.
6. 자동으로 추가된 노드에 설치된 스피커가 데몬셋이 맞는지 확인한다. 명령어에서 스피커 이름은 각자 생성된 이름으로 넣는다.

## ✅ 컨피그맵
`컨피그맵 (ConfigMap)` 은 이름 그대로 설정(config)을 목적으로 사용하는 오브젝트이다.
실습으로 컨피그맵으로 작성된 MetalLB 의 IP 설정을 변경해 본다.

```shell
# m-k8s at super putty
kubectl create deployment cfgmap --image=sysnet4admin/echo-hname
kubectl expose deployment cfgmap --type=LoadBalancer --name=cfgmap-svc --port=80
kubectl get services
cat ~/_Book_k8sInfra/ch3/3.4.2/metallb-l2config.yaml | grep 192.
sed -i 's/11/21;s/13/23/' ~/_Book_k8sInfra/ch3/3.4.2/metallb-l2config.yaml
cat ~/_Book_k8sInfra/ch3/3.4.2/metallb-l2config.yaml | grep 192.
kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.2/metallb-l2config.yaml
kubectl delete pods --all -n metallb-system
kubectl get pods -n metallb-system
kubectl delete service cfgmap-svc
kubectl expose deployment cfgmap --type=LoadBalancer --name=cfgmap-svc --port=80
kubectl get services
```
1. 테스트용 디플로이먼트를 cfgmap이라는 이름으로 생성한다.
2. cfgmap을 로드밸런서(MetalLB)를 통해 노출하고 이름은 cfgmap-svc로 지정한다.
3. 생성된 서비스의 IP(192.168.1.11)을 확인한다.
4. 사전에 구성돼 있는 컨피그맵의 기존 IP(192.168.1.11 ~ 192.168.1.13)를 sed 명령을 사용해 192.168.1.21 ~ 192.168.1.23 으로 변경한다.
5. 사전에 구성돼 있는 컨피그맵의 기존 IP(192.168.1.11 ~ 192.168.1.13)를 sed 명령을 사용해 192.168.1.21 ~ 192.168.1.23 으로 변경한다.
6. 사전에 구성돼 있는 컨피그맵의 기존 IP(192.168.1.11 ~ 192.168.1.13)를 sed 명령을 사용해 192.168.1.21 ~ 192.168.1.23 으로 변경한다.
7. 컨피그맵 설정 파일에 apply 를 실행해 변경된 설정을 적용한다.
8. MetalLB와 관련된 모든 파드를 삭제한다. 삭제하고 나면 kubelet 에서 해당 파드를 자동으로 모두 다시 생성한다.
9. 새로 생성된 MetalLB의 파드들을 확인한다.
10. 기존에 노출한 MetalLB 서버스를 삭제한다.
11. 동일한 이름으로 다시 생성해 새로운 컨피그맵을 적용한 서비스가 올라오게 한다.
12. 변경된 설정이 적용돼 새로운 MetalLB 서비스의 IP 가 192.168.1.21. 로 바뀌었는지 확인한다.

```http request
192.168.1.21
```
13. 192.168.1.21 로 접속해 파드의 이름이 화면에 표시되는지 확인한다.

```shell
# m-k8s at super putty
kubectl delete deployment cfgmap
kubectl delete service cfgmap-svc
```
14. 다음 테스트를 위해 생성한 디플로이먼트와 서비스를 삭제한다.
15. 다음 테스트를 위해 생성한 디플로이먼트와 서비스를 삭제한다.

## ✅ PV와 PVC
파드에서 생성한 내용을 기록하고 보관하거나 모든 파드가 동일한 설정 값을 유지하고 관리하기 위해 공유된 볼륨으로부터 공통된 설정을 가지고 올 수 있도록 설계해야 할 때도 있다.
쿠버네티스는 이런 경우를 위해 다음과 같은 목적으로 다양한 형태의 볼륨을 제공한다.
다양한 쿠버네티스 볼륨 스토리지 중에서 PV 와 PVC 를 확인한다.
- `PV (Persistent Volume)`: 지속적으로 사용 가능한 볼륨, 볼륨을 사용할 수 있게 준비하는 단계
- `PVC (Persistent Volume Claim)`:  지속적으로 사용 가능한 볼륨 요청, 준비된 볼륨에서 일정 공간을 할당 받는 것

실습으로 구현하기 쉬운 NFS 볼륨 타입으로 PV와 PVC를 생성하고 파드에 마운트해 보면서 실제로 어떻게 작동하는지 확인한다.

### NFS 볼륨에 PV/PVC를 만들고 파드에 연결하기

```shell
# m-k8s at super putty
mkdir /nfs_shared
echo '/nfs_shared 192.168.1.0/24(rw, sync, no_root_squash)' >> /etc/exports
systemctl enable --now nfs
kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.3/nfs-pv.yaml
kubectl get pv
kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.3/nfs-pvc.yaml
kubectl get pvc
kubectl get pv
kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.3/nfs-pvc-deploy.yaml
kubectl get pods
kubectl exec -it nfs-pvc-deploy-7888b77964-69c8n -- /bin/bash
```

```shell
# [pod] nfs-pvc-deploy-7888b77064-69c8n
df -h
```

```shell
# m-k8s at super putty
kubectl expose deployment nfs-pvc-deploy --type=LoadBalancer
kubectl get services
```

```http request
192.168.1.21
```

```shell
# [pod] nfs-pvc-deploy-7888b77964-69c8n
ls /audit
cat /audit/audit_nfs-pvs-deploy-7888b77964-qwwsm.log
```

```shell
# m-k8s at super putty
kubectl scale deployment nfs-pvc-deploy --replicas=8
kubectl get pods
kubectl exec -it nfs-pvc-deploy-7888b77964-c6nrp -- /bin/bash
```

```http request
192.168.1.21
```

```shell
# [pod] nfs-pvc-deploy-7888b77964-69c8n
ls /audit
cat /audit/audit_nfs-pvc-deploy-7888b77964-mj6mt.log
ls /audit
```

```yaml
# nfs-pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv
spec:
  capacity:
    storage: 100Mi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  nfs:
    server: 192.168.1.10
    path: /nfs_shared
```

```yaml
# nfs-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Mi
```

```yaml
# nfs-pvc-deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-pvc-deploy
spec:
  replicas: 4
  selector:
    matchLabels:
      app: nfs-pvc-deploy
  template:
    metadata:
      labels:
        app: nfs-pvc-deploy
    spec:
      containers:
        - name: audit-trail
          image: sysnet4admin/audit-trail
          volumeMounts:
            - name: nfs-vol
              mountPath: /audit
      volumes:
        - name: nfs-vol
          persistentVolumeClaim:
            claimName: nfs-pvc
```

### NFS 볼륨을 파드에 직접 마운트하기
```shell
# m-k8s at super putty
kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.3/nfs-ip.yaml
kubectl get pods
kubectl exec -it nfs-ip-84fd4d6f69-475vb -- /bin/bash
```

```shell
# [pod] nfs-ip-84fd4d6f69-475vb
ls /audit
```

```shell
# m-k8s at super putty
kubectl delete deployment nfs-pvc-deploy
kubectl delete deployment nfs-ip
kubectl delete service nfs-pvc-deploy-svc
```

### Tip. 볼륨 용량을 제한하는 방법
#### PVC로 PV에 요청되는 용량을 제한하는 방법
```shell
# m-k8s at super putty
kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.3/limits-pvc.yaml
kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.3/nfs-pv.yaml
kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.3/nfs-pvc.yaml
kubectl delete limitranges storagelimits
```

```yaml
# limits-pvc.yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: storagelimits
spec:
  limits:
  - type: PersistentVolumeClaim
    max:
      storage: 5Mi
    min:
      storage: 1Mi
```

#### 스토리지 리소스에 대한 총 용량을 제한하는 방법
```shell
# m-k8s at super putty
kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.3/quota-pvc.yaml
kubectl get pv
kubectl apply -f nfs-pvc.yaml
kubectl apply -f nfs-pvc1.yaml
kubectl apply -f nfs-pvc2.yaml
kubectl delete resourcequotas storagequota
kubectl delete pvc nfs-pvc1
kubectl delete pv nfs-pv2
kubectl delete pv nfs-pv1
```

```yaml
# quota-pvc.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: storagequota
spec:
  hard:
    persistentvolumeclaims: "5"
    requests.storage: "25Mi"
```

## ✅ 스테이트풀셋

