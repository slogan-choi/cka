# 알아두면 쓸모있는 쿠버네티스 오브젝트
지금까지 파드를 안정적으로 사용하는 방법을 배우며 파드를 관리하는 여러 가지 기능이 포함된 디플로이먼트 오브젝트를 사용해 봤다.
디플로이먼트 외에도 용도에 따라 사용할 수 있는 다양한 오브젝트가 이미 정의돼 있다. (데몬셋, 컨피그맵, PV, PVC, 스테이트풀셋 등)

## ✅ 데몬셋
`데몬셋 (DaemonSet)`: 디플로이먼트의 replicas 가 노드 수만큼 정해져 있는 형태라고 할 수 있는데, 노드 하나당 파드 한 개만을 생성한다. 
결국 노드를 관리하는 파드라면 데몬셋으로 만드는 게 가장 효율적이다.

실습으로 데몬셋을 만들어 보면서 데몬셋의 작동 원리를 확인한다.

```shell
# m-k8s at super putty
kubectl get pods -n metallb-system -o wide
```
1. 현재 MetalLB의 스피커가 각 노드에 분포돼 있는 상태를 확인한다.

```shell
# Vagrantfile
Vagrant.configure("2") do |config|
  N = 4 # max number of worker nodes
  Ver = '1.18.4' # Kubernetes Version to install

  #=============#
  # Master Node #
  #=============#
```
2. 워커 노드를 4개로 변경한다.

```shell
# CMD of host computer
cd C:\HashiCorp\_Book_k8sInfra-main\ch3\3.1.3 
vagrant up w4-k8s
```
3. 호스트 컴퓨터 명령 창에서 C:\HashiCorp\_Book_k8sInfra-main\ch3\3.1.3 경로로 이동한다.
4. 새로운 워커노드를 추가하는 명령을 실행한다.

```shell
# m-k8s at super putty
kubectl get pods -n metallb-system -o wide -w
kubectl get pods speaker-vnc2k -o yaml -n metallb-system
```
5. w4-k8s 가 추가되면 m-k8s 에서 kubectl get pods -n metallb-system -o wide -w 를 수행한다.
6. 자동으로 추가된 노드에 설치된 스피커가 데몬셋이 맞는지 확인한다. 명령어에서 스피커 이름은 각자 생성된 이름으로 넣는다.

## ✅ 컨피그맵
`컨피그맵 (ConfigMap)` 은 이름 그대로 설정(config)을 목적으로 사용하는 오브젝트이다.
실습으로 컨피그맵으로 작성된 MetalLB 의 IP 설정을 변경해 본다.

```shell
# m-k8s at super putty
kubectl create deployment cfgmap --image=sysnet4admin/echo-hname
kubectl expose deployment cfgmap --type=LoadBalancer --name=cfgmap-svc --port=80
kubectl get services
cat ~/_Book_k8sInfra/ch3/3.4.2/metallb-l2config.yaml | grep 192.
sed -i 's/11/21;s/13/23/' ~/_Book_k8sInfra/ch3/3.4.2/metallb-l2config.yaml
cat ~/_Book_k8sInfra/ch3/3.4.2/metallb-l2config.yaml | grep 192.
kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.2/metallb-l2config.yaml
kubectl delete pods --all -n metallb-system
kubectl get pods -n metallb-system
kubectl delete service cfgmap-svc
kubectl expose deployment cfgmap --type=LoadBalancer --name=cfgmap-svc --port=80
kubectl get services
```
1. 테스트용 디플로이먼트를 cfgmap이라는 이름으로 생성한다.
2. cfgmap을 로드밸런서(MetalLB)를 통해 노출하고 이름은 cfgmap-svc로 지정한다.
3. 생성된 서비스의 IP(192.168.1.11)을 확인한다.
4. 사전에 구성돼 있는 컨피그맵의 기존 IP(192.168.1.11 ~ 192.168.1.13)를 sed 명령을 사용해 192.168.1.21 ~ 192.168.1.23 으로 변경한다.
5. 사전에 구성돼 있는 컨피그맵의 기존 IP(192.168.1.11 ~ 192.168.1.13)를 sed 명령을 사용해 192.168.1.21 ~ 192.168.1.23 으로 변경한다.
6. 사전에 구성돼 있는 컨피그맵의 기존 IP(192.168.1.11 ~ 192.168.1.13)를 sed 명령을 사용해 192.168.1.21 ~ 192.168.1.23 으로 변경한다.
7. 컨피그맵 설정 파일에 apply 를 실행해 변경된 설정을 적용한다.
8. MetalLB와 관련된 모든 파드를 삭제한다. 삭제하고 나면 kubelet 에서 해당 파드를 자동으로 모두 다시 생성한다.
9. 새로 생성된 MetalLB의 파드들을 확인한다.
10. 기존에 노출한 MetalLB 서버스를 삭제한다.
11. 동일한 이름으로 다시 생성해 새로운 컨피그맵을 적용한 서비스가 올라오게 한다.
12. 변경된 설정이 적용돼 새로운 MetalLB 서비스의 IP 가 192.168.1.21. 로 바뀌었는지 확인한다.

```http request
192.168.1.21
```
13. 192.168.1.21 로 접속해 파드의 이름이 화면에 표시되는지 확인한다.

```shell
# m-k8s at super putty
kubectl delete deployment cfgmap
kubectl delete service cfgmap-svc
```
14. 다음 테스트를 위해 생성한 디플로이먼트와 서비스를 삭제한다.
15. 다음 테스트를 위해 생성한 디플로이먼트와 서비스를 삭제한다.

## ✅ PV와 PVC
파드에서 생성한 내용을 기록하고 보관하거나 모든 파드가 동일한 설정 값을 유지하고 관리하기 위해 공유된 볼륨으로부터 공통된 설정을 가지고 올 수 있도록 설계해야 할 때도 있다.
쿠버네티스는 이런 경우를 위해 다음과 같은 목적으로 다양한 형태의 볼륨을 제공한다.
다양한 쿠버네티스 볼륨 스토리지 중에서 PV 와 PVC 를 확인한다.
- `PV (Persistent Volume)`: 지속적으로 사용 가능한 볼륨, 볼륨을 사용할 수 있게 준비하는 단계
- `PVC (Persistent Volume Claim)`:  지속적으로 사용 가능한 볼륨 요청, 준비된 볼륨에서 일정 공간을 할당 받는 것

> 실제로 PV와 PVC를 구성해서 PV와 PVC를 구성하는 주체가 관리자와 사용자로 나뉜다는 것을 확인한다.
> 또한 관리자와 사용자가 나뉘어 있지 않다면 굳이 PV와 PVC를 통하지 않고 바로 파드에 공유가 가능한 NFS 볼륨을 마운트할 수 있음을 확인한다.

실습으로 구현하기 쉬운 NFS 볼륨 타입으로 PV와 PVC를 생성하고 파드에 마운트해 보면서 실제로 어떻게 작동하는지 확인한다.

### NFS 볼륨에 PV/PVC를 만들고 파드에 연결하기

```shell
# m-k8s at super putty
mkdir /nfs_shared
echo '/nfs_shared 192.168.1.0/24(rw, sync, no_root_squash)' >> /etc/exports
systemctl enable --now nfs
kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.3/nfs-pv.yaml
kubectl get pv
kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.3/nfs-pvc.yaml
kubectl get pvc
kubectl get pv
kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.3/nfs-pvc-deploy.yaml
kubectl get pods
kubectl exec -it nfs-pvc-deploy-7888b77964-69c8n -- /bin/bash
```
1. PV 로 선언할 볼륨을 만들기 위해 NFS 서버를 마스터 노드에 구성한다. 공유되는 디렉터리는 /nfs_shared 로 생성한다.
2. 해당 디렉터리를 NFS로 받아들일 IP 영역은 192.168.1.0/24 로 정한다. 옵션을 적용해 /etc/exports 에 기록한다.
    - 옵션 `rw`: 읽기/쓰기
    - 옵션 `sync`: 쓰기 작업 동기화
    - 옵션 `no_root_squash`: root 계정 사용
3. 해당 내용을 시스템에 적용해 NFS 서버를 활성화하고 다음에 시작할 때도 자동으로 적용되도록 한다.
4. 오브젝트 스펙을 실행해 PV 를 생성한다.
5. 생성된 PV의 상태가 Available(사용 가능) 임을 확인한다.
6. 오브젝트 스펙을 실행해 PVC 를 생성한다.
7. 생성된 PVC 를 확인한다.
    - STATUS: `Bound (묶여짐)` (PV 와 PVC 가 연결됐음을 의미)
    - CAPACITY: 용량은 동적으로 PVC를 따로 요청해 생성하는 경우가 아니면 큰 의미가 없다.
8. PV 의 상태도 Bound 로 바뀌었음을 확인한다.
9. 생성한 PVC 를 볼륨으로 사용하는 디플로이먼트 오브젝트 스펙을 배포한다.
10. 생성된 파드를 확인한다.
11. 생성한 파드 중 하나에 exec로 접속한다.

```shell
# [pod] nfs-pvc-deploy-7888b77064-69c8n
df -h
```
12. PVC의 마운트 상태를 확인한다.

```shell
# m-k8s at super putty
kubectl expose deployment nfs-pvc-deploy --type=LoadBalancer
kubectl get services
```
13. 오른쪽에 m-k8s 명령 창을 1개 더 열고 audit-trail 컨테이너의 기능을 테스트한다. 외부에서 파드(nfs-pv-deploy)에 접속할 수 있도록 expose 로 로드밸런서 서비스를 생성한다.
14. 생성한 로드밸런서 서비스의 IP를 확인한다.

```http request
192.168.1.21
```
15. 호스트 컴퓨터에서 브라우저를 연다. 192.168.1.21 에 접속해 파드 이름과 IP가 표시되는지 확인한다.

```shell
# [pod] nfs-pvc-deploy-7888b77964-69c8n
ls /audit
cat /audit/audit_nfs-pvs-deploy-7888b77964-qwwsm.log
```
16. exec를 통해 접속한 파드에서 ls /audit 명령을 실행해 접속 기록 파일이 남았는지 확인한다.
17. cat 으로 해당 파일의 내용도 함께 확인한다.

```shell
# m-k8s at super putty
kubectl scale deployment nfs-pvc-deploy --replicas=8
kubectl get pods
kubectl exec -it nfs-pvc-deploy-7888b77964-c6nrp -- /bin/bash
```
18. 마스터 노드(m-k8s)에서 scale 명령으로 파드를 4개에서 8개로 증가시킨다.
19. 생성된 파드를 확인한다.
20. 최근에 증가한 4개의 파드 중 1개를 선택해 exec 로 접속하고 기록된 audit 로그가 동일한지 확인한다.

```http request
192.168.1.21
```
21. 다른 브라우저를 열고 192.168.1.21 로 접속해 다른 파드 이름과 IP가 표시되는지를 확인한다.

```shell
# [pod] nfs-pvc-deploy-7888b77964-69c8n
ls /audit
cat /audit/audit_nfs-pvc-deploy-7888b77964-mj6mt.log
ls /audit
```
22. exec로 접속한 파드에서 ls /audit 을 실행해 새로 추가된 audit 로그를 확인한다.
23. cat 으로 기록된 내용도 함께 확인한다.
24. 기존에 접속한 파드에서도 동일한 로그가 audit 에 기록돼 있는지 확인한다.

```yaml
# nfs-pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv
spec:
  capacity:
    storage: 100Mi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  nfs:
    server: 192.168.1.10
    path: /nfs_shared
```
- spec.capacity.storage: 실제로 사용하는 용량을 제한하는 것이 아니라 쓸 수 있는 양을 레이블로 붙이는 것과 같다. (현재 스토리지가 단순히 NFS로 설정돼서 그렇다.)
- spec.accessModes: PV를 어떤 방식으로 사용할지를 정의한 부분이다.
    - `ReadWriteOnce`: 하나의 노드에서만 볼륨을 읽고 쓸 수 있게 마운트
    - `ReadOnlyMany`: 여러 개의 노드가 읽도록 마운트
- spec.persistentVolumeReclaimPolicy: PV 가 제거됐을 때 작동하는 방법을 정의하는 것
    - `Retain`: 유지
    - `Delete`: 삭제
    - `Recycle`: 재활용, Deprecated
- spec.nfs: NFS 서버의 연결 위치에 대한 설정

```yaml
# nfs-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Mi
```
- PVC 는 PV 와 구성이 거의 동일하다. 하지만 PV 는 사용자가 요청할 볼륨 공간을 관리자가 만들고, PVC 는 사용자(개발자)간 볼륨을 요청하는 데 사용한다는 점에서 차이가 있다.

```yaml
# nfs-pvc-deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-pvc-deploy
spec:
  replicas: 4
  selector:
    matchLabels:
      app: nfs-pvc-deploy
  template:
    metadata:
      labels:
        app: nfs-pvc-deploy
    spec:
      containers:
        - name: audit-trail
          image: sysnet4admin/audit-trail
          volumeMounts:
            - name: nfs-vol
              mountPath: /audit
      volumes:
        - name: nfs-vol
          persistentVolumeClaim:
            claimName: nfs-pvc
```
- spec.template.spec.containers: audit-trail 이미지를 가지고 온다. 해당 컨테이너 이미지는 요청을 처리할 때마다 접속 정보를 로그로 기록한다.
- spec.template.spec.containers.volumeMounts: 볼륨이 마운트도리 위치(/audit)를 지정한다.
- spec.template.spec.volumes: PVC 로 생성된 볼륨을 마운트하기 위해서 nfs-pvc 라는 이름을 사용한다.

### NFS 볼륨을 파드에 직접 마운트하기
```shell
# m-k8s at super putty
kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.3/nfs-ip.yaml
kubectl get pods
kubectl exec -it nfs-ip-84fd4d6f69-475vb -- /bin/bash
```
1. 사용자가 관리자와 동일한 단일 시스템이라면 PV와 PVC를 사용할 필요가 없다. 따라서 단순히 볼륨을 마운트하는지 확인하고 넘어간다.
2. 새로 배포된 파드를 확인한다.
3. 그중 하나에 exec로 접속한다.

```shell
# [pod] nfs-ip-84fd4d6f69-475vb
ls /audit
```
4. 접속한 파드에서 ls /audit 를 실행해 동일한 NFS 볼륨을 바라보고 있음을 확인한다.

```shell
# m-k8s at super putty
kubectl delete deployment nfs-pvc-deploy
kubectl delete deployment nfs-ip
kubectl delete service nfs-pvc-deploy-svc
```
5. 다음 진행을 위해 설치한 PV와 PVC를 제외한 나머지인 파드와 서비스를 삭제한다.
6. 다음 진행을 위해 설치한 PV와 PVC를 제외한 나머지인 파드와 서비스를 삭제한다.
7. 다음 진행을 위해 설치한 PV와 PVC를 제외한 나머지인 파드와 서비스를 삭제한다.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-ip
spec:
  replicas: 4
  selector:
    matchLabels:
      app: nfs-ip
  template:
    metadata:
      labels:
        app: nfs-ip
    spec:
      containers:
      - name: audit-trail
        image: sysnet4admin/audit-trail
        volumeMounts:
        - name: nfs-vol
          mountPath: /audit
      volumes:
      - name: nfs-vol
        nfs:
          server: 192.168.1.10
          path: /nfs_shared
```
- spec.template.spec.volumes: PV와 PVC를 거치지 않고 바로 NFS 서버로 접속하는 것을 확인할 수 있다.

### Tip. 볼륨 용량을 제한하는 방법
#### PVC로 PV에 요청되는 용량을 제한하는 방법
```shell
# m-k8s at super putty
kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.3/limits-pvc.yaml
kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.3/nfs-pv.yaml
kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.3/nfs-pvc.yaml
kubectl delete limitranges storagelimits
```
1. PVC로 PV를 요청할 때 용량을 제한하는 오브젝트 스펙을 가져와 적용한다.
2. PV와 PVC를 새로 생성하고 PVC가 최대 용량 제한(5Mi)에 걸려 수행되지 못하는지 확인한다.
3. PV와 PVC를 새로 생성하고 PVC가 최대 용량 제한(5Mi)에 걸려 수행되지 못하는지 확인한다.
4. 용량 제한 설정을 삭제한다.

```yaml
# limits-pvc.yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: storagelimits
spec:
  limits:
  - type: PersistentVolumeClaim
    max:
      storage: 5Mi
    min:
      storage: 1Mi
```
- PVC를 통해 PV를 요청할 때 최소 1Mi에서 최대 5Mi로 용량를 제한한다.

#### 스토리지 리소스에 대한 총 용량을 제한하는 방법
```shell
# m-k8s at super putty
kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.3/quota-pvc.yaml
kubectl get pv
kubectl apply -f nfs-pvc.yaml
kubectl apply -f nfs-pvc1.yaml
kubectl apply -f nfs-pvc2.yaml
kubectl delete resourcequotas storagequota
kubectl delete pvc nfs-pvc1
kubectl delete pv nfs-pv2
kubectl delete pv nfs-pv1
```
1. 총 누적 사용량을 제한하기 위해 quota-pvc.yaml 오브젝트 스펙을 적용한다.
2. PV 3개(100Mi)의 상태를 만들고 오브젝트 스펙을 작성한다. PVC3개(10Mi)를 요청해 25Mi 제한으로 더 이상 PVC 가 수행되지 못하는지 확인한다.
3. PV 3개(100Mi)의 상태를 만들고 오브젝트 스펙을 작성한다. PVC3개(10Mi)를 요청해 25Mi 제한으로 더 이상 PVC 가 수행되지 못하는지 확인한다.
4. PV 3개(100Mi)의 상태를 만들고 오브젝트 스펙을 작성한다. PVC3개(10Mi)를 요청해 25Mi 제한으로 더 이상 PVC 가 수행되지 못하는지 확인한다.
5. PVC를 생성하기 위해 설정한 리소스 제한을 삭제한다.
6. 과도하게 생성한 PV와 PVC를 삭제한다. 이때 Bound의 상대를 잘보고 삭제해야 한다.
7. 과도하게 생성한 PV와 PVC를 삭제한다. 이때 Bound의 상대를 잘보고 삭제해야 한다.
8. 과도하게 생성한 PV와 PVC를 삭제한다. 이때 Bound의 상대를 잘보고 삭제해야 한다.

```yaml
# quota-pvc.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: storagequota
spec:
  hard:
    persistentvolumeclaims: "5"
    requests.storage: "25Mi"
```
- 해당 코드는 PVC는 5개, 용량은 25Mi가 넘지 않도록 제한한다.

## ✅ 스테이트풀셋
- 지금까지는 파드가 replicas 에 선언된 만큼 무작위로 생성될 뿐이었다. 그런데 파드가 만들어지는 이름과 순서를 예측해야 할 때가 있다.
주로 레디스(Redis), 주키퍼(Zookeeper), 카산드라(Cassandra), 몽고DB(MongoDB) 등의 마스터-슬레이브 구조 시스템에서 필요하다.
- `스테이트풀셋 (StatefulSet)`: volumeClaimTemplate 기능을 사용해 PVC를 자동으로 생성할 수 있고, 각 파드가 순서대로 생성되기 때문에 고정된 이름, 볼륨, 설정 등을 가질 수 있다.
다만, 효율성 면에서 좋은 구조가 아니므로 요구 사항에 맞게 적절히 사용하는 것이 좋다.
  
실습으로 스테이트풀셋을 직접 만들어 보면서 생성 과정을 살펴보고 어떤 형태의 고정 값을 가지는지 확인한다.
```shell
# m-k8s at super putty
kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.4/nfs-pvc-sts.yaml
kubectl get pods -w
kubectl expose statefulset nfs-pvc-sts --type=LoadBalancer --name=nfs-pvs-sts-svc --port=80
kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.5/nfs-pvc-sts-svc.yaml
kubectl get services
```


```http request
192.168.1.21
```

```shell
# m-k8s at super putty
kubectl exec -it nfs-pvc-sts-0 -- /bin/bash
```

```shell
# [pod] nfs-pvc-sts
ls -l /audit
exit
```

```shell
# m-k8s at super putty
kubectl delete statefulset nfs-pvc-sts
kubectl get pods -w
```

### Note. 스테이트풀셋은 헤드리스(Headless) 서비스로 노출한다고 하던데요?
- `헤드리스 서비스`: IP를 가지지 않는 서비스 타입으로 중요한 자원인 IP를 절약할 수 있을 뿐만 아니라, 스테이트풀셋과 같은 상태를 가지고 있는 오브젝트를 모두 노출하지 않고 상태 값을 외부에 알리고 싶은 것만 선택적으로 노출하게 할 수 있다.
- 일반적으로는 스테이트풀셋은 헤드리스 서비스로 노출한다. 그러나 고정된 이름을 사용하면서 외부에 모든 스테이트풀셋을 노출하고자 하는 경우에는 노드포트나 로드밸런서 서비스로 노출할 수 있다.

```shell
# m-k8s at super putty
kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.4/sts-svc-domain.yaml
kubectl get services
kubectl run net --image=sysnet4admin/net-tools --restart=Never --rm -it -- nslookup nfs-pvc-sts-0.sts-svc.domin
kubectl delete services sts-svc-domain
```

### Note. 클라우드 스토리지에서 PV와 PVC가 동적으로 할당되는 예제 (실습 환경: EKS, GKE, AKS)
클라우드의 스토리지와 오브젝트 형태의 스토리지는 동적으로 PVC를 요청을 받아서 처리할 수 있도록 구현돼 있다. 이때 오브젝트는 kind: StorageClass를 사용하고, PV와 PVC가 오브젝트를 호출하는 구조이다.
여기서 standard.yaml 과 같이 StorageClass를 선언한다. 그리고 메타데이터로 지정한 standard로 호출이 들어오면 동적으로 스토리지를 제공한다.

```shell
kubectl apply -f \
https://raw.githubusercontent.com/sysnet4admin/_Book_k8sInfra/main/ch3/3.4.4/dynamic-pvc.yaml
kubectl get pvc
kubectl get pv
kubectl apply -f \
https://github.com/sysnet4admin/_Book_k8sInfra/blob/main/ch3/3.4.4/dynamic-pvc-deploy.yaml
kubectl get pods
kubectl exec -it dynamic-pvc-deploy-78fc9c89c5-72nv6 -- /bin/bash
```

```shell
# [pod] dynamic-pvc-deploy-78fc9c89c5-72nv6
df -h
```